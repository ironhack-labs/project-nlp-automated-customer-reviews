{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pickle\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = pd.read_csv('./1429_1.csv') # Used for training and fine tunning\n",
    "reviews_test = pd.read_csv('./Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv') # Used for testing\n",
    "#reviews_2 = pd.read_csv('./Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv') DUPLICATE INFORMATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews_train.shape)\n",
    "display(reviews_train.columns)\n",
    "display(reviews_train.sample(1))\n",
    "reviews = reviews_train[['categories', 'reviews.rating', 'reviews.text']]\n",
    "display(reviews.dtypes)\n",
    "# Check for missing values in the DataFrame\n",
    "missing_values = pd.isnull(reviews)\n",
    "# Count missing values in each column\n",
    "missing_counts = missing_values.sum()\n",
    "# Count columns with missing values\n",
    "columns_with_missing = missing_counts[missing_counts > 0].count()\n",
    "# Check if all columns have missing values\n",
    "all_columns_missing = missing_counts.all()\n",
    "# Calculate the total number of missing values\n",
    "total_missing_values = missing_counts.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_counts)\n",
    "print(\"\\nNumber of Columns with Missing Values:\", columns_with_missing)\n",
    "print(\"All Columns Have Missing Values:\", all_columns_missing)\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\", total_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erase rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna().reset_index()\n",
    "reviews_test = reviews_test.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: summarize by Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'category' and 'rating' and concatenate text entries\n",
    "grouped_reviews = reviews.groupby(['categories', 'reviews.rating'])['reviews.text'].apply(lambda x: ' '.join(x)).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import summarization model from Hgging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summarization pipeline with a pre-trained model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summary = []\n",
    "text = grouped_reviews['reviews.text']\n",
    "\n",
    "for sentence in text.values:\n",
    "    summary.append(summarizer(sentence))\n",
    "\n",
    "# Add summarized text to a new column\n",
    "grouped_reviews['summary'] = pd.Series(summary)\n",
    "# Display the result\n",
    "print(grouped_reviews[['category', 'rating', 'summary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export dataframe into csv for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_reviews.to_csv('categories_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change values of column 'reviews.rating' for standardization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews['reviews.rating'] = reviews['reviews.rating'].replace({1.0: 'negative', 2.0: 'negative', 3.0: 'negative', \n",
    "                                               4.0: 'neutral', 5.0: 'positive'})\n",
    "\n",
    "display(reviews['reviews.rating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the information about our cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a smaller sample for efficiency and training purposes. Balance the dataset through the min of value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = reviews['reviews.rating'].value_counts().min()\n",
    "\n",
    "# Filter for each sentiment class\n",
    "positive_reviews = reviews[reviews['reviews.rating'] == 'positive'].sample(n=min_samples, random_state=42, replace=True)\n",
    "negative_reviews = reviews[reviews['reviews.rating'] == 'negative'].sample(n=min_samples, random_state=42, replace=True)\n",
    "neutral_reviews = reviews[reviews['reviews.rating'] == 'neutral'].sample(n=500, random_state=42, replace=True)\n",
    "\n",
    "# Concatenate the sampled DataFrames\n",
    "balanced_reviews = pd.concat([positive_reviews, negative_reviews, neutral_reviews])\n",
    "\n",
    "# Shuffle the resulting DataFrame to mix the classes\n",
    "balanced_reviews = balanced_reviews.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Now balanced_df contains 3000 of each sentiment class\n",
    "print(balanced_reviews['reviews.rating'].value_counts())\n",
    "reviews_sample = balanced_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    \"\"\"\n",
    "    This function processes each setence and applies regex patterns to remove undesired characters.\n",
    "    In this case we built it detele characters that should be equally translated by computers and humans:\n",
    "    - special characters\n",
    "    - numerical characters/digits\n",
    "    - single characthers\n",
    "    - multiple spaces (for cleaning purposes)\n",
    "\n",
    "    Argument: text/corpus/document/sentence; string\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove numbers\n",
    "    text_no_special_characters = re.sub(r'[^A-Za-z\\s]+', ' ', str(text))\n",
    "\n",
    "    # Remove all single characters (e.g., 'a', 'b', 'c' that appear as standalone)\n",
    "    text_no_single_charac = re.sub(r'\\b\\w\\b', '', text_no_special_characters)\n",
    "\n",
    "    # Clean up extra spaces left after removing single characters\n",
    "    text_cleaned = re.sub(r'\\s+', ' ', text_no_single_charac).strip()\n",
    "\n",
    "    # Transform data to lowercase\n",
    "    text_cleaned = text_cleaned.lower()\n",
    "\n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def data_processing(text):\n",
    "    \"\"\"\n",
    "    This function processes each sentence in the following order:\n",
    "    1. Tokenize each word of the sentence.\n",
    "    2. Remove stopwords and stem words, if any word is in the 'stopwords.words(\"english\")' list.\n",
    "    3. Lemmatize every word not in the stopwords list\n",
    "    4. Join all the tokens per row, to rebuild the sentences.\n",
    "\n",
    "    Argument: text/corpus/document/sentence; string\n",
    "    \"\"\"\n",
    "    tolkenize_words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in tolkenize_words if word not in stopwords.words(\"english\")]\n",
    "    text_processed = ' '.join(lemmatized_words)  # Join the words back into a single string\n",
    "\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix using seaborn heatmap.\n",
    "\n",
    "    Args:\n",
    "    y_true: list or array of true labels\n",
    "    y_pred: list or array of predicted labels\n",
    "    labels: list of label names (optional)\n",
    "    normalize: boolean, whether to normalize the confusion matrix\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "                xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "def plot_vec_count_matrix(X_vectorized, vectorizer):\n",
    "    # Convert to DataFrame for easier handling\n",
    "    count_df = pd.DataFrame(X_vectorized.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    term_sums = count_df.sum().sort_values(ascending=False)\n",
    "\n",
    "    # Plot the most common terms\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    term_sums.head(10).plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Top 10 Most Frequent Terms\")\n",
    "    plt.xlabel(\"Terms\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_cleaned = reviews_sample['reviews.text'].apply(data_cleaning)\n",
    "reviews_processed = reviews_cleaned.apply(data_processing)\n",
    "\n",
    "reviews_test_cleaned = reviews_test['reviews.text'].apply(data_cleaning)\n",
    "reviews_test_processed = reviews_test_cleaned.apply(data_processing)\n",
    "\n",
    "reviews_processed.head(), reviews_test_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews_processed\n",
    "y = reviews_sample['reviews.rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of Words model\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_count = CountVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_count = vectorizer_count.transform(X_test)\n",
    "\n",
    "with open('TF-IDF_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "plot_vec_count_matrix(X_train_count, vectorizer_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB with gridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NB = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid_NB = {\n",
    "    'alpha':  [0.21],  # Regularization parameter\n",
    "    'fit_prior': [True]  # Whether to learn class priors\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_NB = GridSearchCV(estimator=model_NB, param_grid=param_grid_NB, \n",
    "                           cv=5, scoring='accuracy', n_jobs=1)\n",
    "\n",
    "# Assuming you have your data X_train and y_train\n",
    "grid_search_NB.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# with open('MultinomialNB.pkl', 'wb') as f:\n",
    "#     pickle.dump(grid_search_NB, f)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search_NB.best_params_)\n",
    "print(\"Best Score: \", grid_search_NB.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NB = grid_search_NB.predict(X_test_tfidf)\n",
    "\n",
    "acc_NB=accuracy_score(y_test, y_pred_NB)\n",
    "print('Accuracy: ', acc_NB)\n",
    "\n",
    "print(\"\\nClassification Report NB:\\n\", classification_report(y_test, y_pred_NB))\n",
    "plot_confusion_matrix(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regession with GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Logistic Regression model\n",
    "model_LR = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid_LR = {\n",
    "    'C': [1, 2],  # Inverse of regularization strength, smaller values mean stronger regularization\n",
    "    'max_iter': [100, 120]  # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_LR = GridSearchCV(estimator=model_LR, \n",
    "                            param_grid=param_grid_LR, \n",
    "                            cv=5, \n",
    "                            scoring='accuracy', \n",
    "                            n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "grid_search_LR.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# with open('LogisticRegression.pkl', 'wb') as f:\n",
    "#     pickle.dump(grid_search_LR, f)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search_LR.best_params_)\n",
    "print(\"Best Score: \", grid_search_LR.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LR = grid_search_LR.predict(X_test_tfidf)\n",
    "\n",
    "acc_LR=accuracy_score(np.asarray(y_test), y_pred_LR)\n",
    "print('Accuracy: ', acc_LR)\n",
    "\n",
    "print(\"\\nClassification Report LR:\\n\", classification_report(np.asarray(y_test), y_pred_LR))\n",
    "plot_confusion_matrix(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC with GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Logistic Regression model\n",
    "model_SVC = SVC()\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid_SVC = {\n",
    "            'kernel': ['linear', \n",
    "            'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_SVC = GridSearchCV(estimator=model_SVC, \n",
    "                    param_grid=param_grid_SVC, \n",
    "                    cv=5, \n",
    "                    scoring='accuracy')\n",
    "\n",
    "# Train the model\n",
    "grid_search_SVC.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# with open('SVC.pkl', 'wb') as f:\n",
    "#     pickle.dump(grid_search_SVC, f)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search_SVC.best_params_)\n",
    "print(\"Best Score: \", grid_search_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVC = grid_search_SVC.predict(X_test_tfidf)\n",
    "\n",
    "acc_SVC = accuracy_score(np.asarray(y_test), y_pred_SVC)\n",
    "print('Accuracy: ', acc_SVC)\n",
    "\n",
    "print(\"\\nClassification Report SVC:\\n\", classification_report(np.asarray(y_test), y_pred_SVC))\n",
    "plot_confusion_matrix(y_test, y_pred_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RandomForestClassifier model\n",
    "model_RF = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid_RF = {\n",
    "    'n_estimators': [100],  # Number of trees in the forest\n",
    "    'max_depth': [2],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_RF = GridSearchCV(estimator=model_RF, \n",
    "                            param_grid=param_grid_RF, \n",
    "                            cv=5, \n",
    "                            scoring='accuracy', \n",
    "                            n_jobs=1, \n",
    "                            verbose=1)\n",
    "\n",
    "#Train the model\n",
    "grid_search_RF.fit(X_train_tfidf, y_train)\n",
    "\n",
    "with open('RandomForest.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search_RF, f)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search_RF.best_params_)\n",
    "print(\"Best Score: \", grid_search_RF.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF = grid_search_RF.predict((X_test_tfidf))\n",
    "\n",
    "acc_RF=accuracy_score(np.asarray(y_test), y_pred_RF)\n",
    "print('Accuracy: ', accuracy_score)\n",
    "\n",
    "print(\"\\nClassification Report RF:\\n\", classification_report(y_test, y_pred_RF))\n",
    "plot_confusion_matrix(y_test, y_pred_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline PreTrained Model without Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model\n",
    "model_name='cardiffnlp/twitter-roberta-base-sentiment'\n",
    "\n",
    "# Instanciate Classifier\n",
    "classifier = pipeline('sentiment-analysis', model=model_name, truncation=True, padding=True)\n",
    "\n",
    "# Execute Classifier\n",
    "ratings = classifier(reviews_cleaned.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_cleaned)\n",
    "len(reviews_sample['reviews.rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipiline with Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [rating['label'] for rating in ratings]\n",
    "predicted_labels_pipeline_no_tun = list(map(lambda label: 'negative' if label == 0 else\n",
    "                                      'neutral' if label == 1 else\n",
    "                                      'positive' if label == 2 else\n",
    "                                      'negative', ratings))\n",
    "\n",
    "#true_labels = reviews_sample['reviews.rating']\n",
    "acc_pipeline_no_tun = accuracy_score(np.asarray(reviews_sample['reviews.rating']), predicted_labels_pipeline_no_tun)\n",
    "print('Accuracy: ', acc_pipeline_no_tun)\n",
    "\n",
    "print(\"\\nClassification Report Pipeline Model:\\n\", classification_report(reviews_sample['reviews.rating'], predicted_labels_pipeline_no_tun))\n",
    "plot_confusion_matrix(reviews_sample['reviews.rating'], predicted_labels_pipeline_no_tun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Data Transformation for label compatibility in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### transformation required for pipiline training ###\n",
    "# Label inputs must be integers\n",
    "labels_for_pip_with_tun = reviews_sample['reviews.rating'].replace({'negative':2, 'neutral': 1, 'positive':0})\n",
    "\n",
    "# Repeat Train and Test set split\n",
    "X = reviews_processed\n",
    "y = labels_for_pip_with_tun\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "model_pretrained = RobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment', num_labels=3)\n",
    "\n",
    "# Tokenize the inputs\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, return_tensors=None)\n",
    "val_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, return_tensors=None)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'label': y_train.tolist()\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'label': y_test.tolist()\n",
    "})\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,   # batch size per device during training\n",
    "    per_device_eval_batch_size=32,    # batch size for evaluation\n",
    "    weight_decay=0.01,                # strength of weight decay\n",
    "    evaluation_strategy=\"epoch\"     # evaluate every epoch\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model_pretrained,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trainer's predict method\n",
    "predictions_pipeline_with_tun = trainer.predict(val_dataset)\n",
    "\n",
    "# softmax the probabilities\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probs = softmax(torch.tensor(predictions_pipeline_with_tun.predictions))\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "predicted_labels_tensor = torch.argmax(probs, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace output labels from integers to positive, negative, and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_pipeline_with_tun = list(map(lambda label: 'negative' if label == 2 else\n",
    "                                      'neutral' if label == 1 else\n",
    "                                      'positive' if label == 0 else\n",
    "                                      'negative', predicted_labels_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine input labels from integers to positive, negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels =  list(map(lambda label: 'negative' if label == 2 else\n",
    "                                      'neutral' if label == 1 else\n",
    "                                      'positive' if label == 0 else\n",
    "                                      'negative', y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_labels = reviews_sample['reviews.rating']\n",
    "acc_pipeline_with_tun = accuracy_score(np.asarray(true_labels), predicted_labels_pipeline_with_tun)\n",
    "print('Accuracy: ', acc_pipeline_with_tun)\n",
    "\n",
    "print(\"\\nClassification Report Pipeline Model:\\n\", classification_report(true_labels, predicted_labels_pipeline_with_tun))\n",
    "plot_confusion_matrix(true_labels, predicted_labels_pipeline_with_tun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
