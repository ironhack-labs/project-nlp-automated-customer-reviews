{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e6f4dc-f076-418f-81ee-1af2430f862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sylviaperez-\n",
      "[nltk_data]     montero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/sylviaperez-\n",
      "[nltk_data]     montero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sylviaperez-\n",
      "[nltk_data]     montero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54682ddb6e5d4ba4996ab422ac4525ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10389' max='10389' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10389/10389 2:15:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Class Metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.258590</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>0.901728</td>\n",
       "      <td>0.871848</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>{'Negative': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Neutral': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Positive': {'Precision': 0.9337279815189142, 'Recall': 1.0, 'F1-score': 0.9657283655641007}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.258755</td>\n",
       "      <td>0.935749</td>\n",
       "      <td>0.914442</td>\n",
       "      <td>0.894192</td>\n",
       "      <td>0.935749</td>\n",
       "      <td>{'Negative': {'Precision': 0.38513513513513514, 'Recall': 0.3433734939759036, 'F1-score': 0.3630573248407643}, 'Neutral': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Positive': {'Precision': 0.9477722041900265, 'Recall': 0.9933508582031854, 'F1-score': 0.9700264250660626}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.234480</td>\n",
       "      <td>0.936038</td>\n",
       "      <td>0.920671</td>\n",
       "      <td>0.911460</td>\n",
       "      <td>0.936038</td>\n",
       "      <td>{'Negative': {'Precision': 0.4714285714285714, 'Recall': 0.39759036144578314, 'F1-score': 0.43137254901960786}, 'Neutral': {'Precision': 0.25757575757575757, 'Recall': 0.05802047781569966, 'F1-score': 0.0947075208913649}, 'Positive': {'Precision': 0.9523809523809523, 'Recall': 0.9896397092933354, 'F1-score': 0.9706529157503602}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Trainer is attempting to log a value of \"{'Negative': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Neutral': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Positive': {'Precision': 0.9337279815189142, 'Recall': 1.0, 'F1-score': 0.9657283655641007}}\" of type <class 'dict'> for key \"eval/class_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/sylviaperez-montero/.pyenv/versions/3.10.12/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Trainer is attempting to log a value of \"{'Negative': {'Precision': 0.38513513513513514, 'Recall': 0.3433734939759036, 'F1-score': 0.3630573248407643}, 'Neutral': {'Precision': 0.0, 'Recall': 0.0, 'F1-score': 0.0}, 'Positive': {'Precision': 0.9477722041900265, 'Recall': 0.9933508582031854, 'F1-score': 0.9700264250660626}}\" of type <class 'dict'> for key \"eval/class_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'Negative': {'Precision': 0.4714285714285714, 'Recall': 0.39759036144578314, 'F1-score': 0.43137254901960786}, 'Neutral': {'Precision': 0.25757575757575757, 'Recall': 0.05802047781569966, 'F1-score': 0.0947075208913649}, 'Positive': {'Precision': 0.9523809523809523, 'Recall': 0.9896397092933354, 'F1-score': 0.9706529157503602}}\" of type <class 'dict'> for key \"eval/class_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'Negative': {'Precision': 0.4714285714285714, 'Recall': 0.39759036144578314, 'F1-score': 0.43137254901960786}, 'Neutral': {'Precision': 0.25757575757575757, 'Recall': 0.05802047781569966, 'F1-score': 0.0947075208913649}, 'Positive': {'Precision': 0.9523809523809523, 'Recall': 0.9896397092933354, 'F1-score': 0.9706529157503602}}\" of type <class 'dict'> for key \"eval/class_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DistilBERT Performance:\n",
      "Model achieved an accuracy of 93.60% on the validation dataset.\n",
      "Class Negative: Precision=47.14%, Recall=39.76%, F1-score=43.14%\n",
      "Class Neutral: Precision=25.76%, Recall=5.80%, F1-score=9.47%\n",
      "Class Positive: Precision=95.24%, Recall=98.96%, F1-score=97.07%\n",
      "\n",
      "✅ Confusion Matrix:\n",
      "[[  66   12   88]\n",
      " [  44   17  232]\n",
      " [  30   37 6400]]\n"
     ]
    }
   ],
   "source": [
    "# Imports the necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "import json\n",
    "import pyarrow as pa\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset as HFDataset, Features, Value\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Makes sure the required NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Loads the dataset\n",
    "df = pd.read_csv(\"/Users/sylviaperez-montero/Desktop/Project/Amazon Data.csv\", low_memory=False)\n",
    "pickle.dump(df, open(\"dataset.pkl\", \"wb\"))\n",
    "\n",
    "# Drops unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'asins', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', \n",
    "    'reviews.id', 'reviews.didPurchase', 'name',\n",
    "    'reviews.userCity', 'reviews.userProvince', 'reviews.sourceURLs'\n",
    "]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "pickle.dump(df, open(\"cleaned_dataset.pkl\", \"wb\"))\n",
    "\n",
    "# Defines text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Removes numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Removes punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Removes extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    stop_words = set(stopwords.words(\"english\"))  # Removes stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()  # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Applies text preprocessing\n",
    "if 'reviews.text' in df.columns:\n",
    "    df[\"cleaned_reviews_text\"] = df[\"reviews.text\"].apply(preprocess_text)\n",
    "\n",
    "# Drops rows with missing values in relevant columns\n",
    "df = df.dropna(subset=[\"cleaned_reviews_text\", \"reviews.rating\"])\n",
    "\n",
    "# Converts 'reviews.rating' to integer\n",
    "df[\"reviews.rating\"] = df[\"reviews.rating\"].astype(int)\n",
    "\n",
    "# Label transformations (1-2 → Negative, 3 → Neutral, 4-5 → Positive)\n",
    "def transform_labels(rating):\n",
    "    return 0 if rating <= 2 else (1 if rating == 3 else 2)\n",
    "\n",
    "df[\"label\"] = df[\"reviews.rating\"].apply(transform_labels)\n",
    "\n",
    "# ConvertS to Hugging Face Dataset\n",
    "features = Features({\n",
    "    \"cleaned_reviews_text\": Value(\"string\"),\n",
    "    \"label\": Value(\"int32\")  \n",
    "})\n",
    "\n",
    "hf_dataset = HFDataset.from_pandas(df[[\"cleaned_reviews_text\", \"label\"]], features=features, preserve_index=False)\n",
    "\n",
    "# Tokenizer Initialization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_reviews_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Applies Tokenization\n",
    "hf_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-Test Split\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Loads the pretrained DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Defines training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_results\",\n",
    "    eval_strategy=\"epoch\",  # ✅ FIXED\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Defines performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Defines performance metrics\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Computes overall metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    recall = recall_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    # Computes per-class metrics\n",
    "    class_report = classification_report(labels, preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"], output_dict=True)\n",
    "\n",
    "    # Extracts per-class precision, recall, and F1-score\n",
    "    class_metrics = {\n",
    "        \"Negative\": {\"Precision\": class_report[\"Negative\"][\"precision\"], \"Recall\": class_report[\"Negative\"][\"recall\"], \"F1-score\": class_report[\"Negative\"][\"f1-score\"]},\n",
    "        \"Neutral\": {\"Precision\": class_report[\"Neutral\"][\"precision\"], \"Recall\": class_report[\"Neutral\"][\"recall\"], \"F1-score\": class_report[\"Neutral\"][\"f1-score\"]},\n",
    "        \"Positive\": {\"Precision\": class_report[\"Positive\"][\"precision\"], \"Recall\": class_report[\"Positive\"][\"recall\"], \"F1-score\": class_report[\"Positive\"][\"f1-score\"]}\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"f1\": f1, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"class_metrics\": class_metrics\n",
    "    }\n",
    "    \n",
    "# Trainer setup \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset[\"train\"],\n",
    "    eval_dataset=hf_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics  # FIXED\n",
    ")\n",
    "\n",
    "# Trains the model\n",
    "trainer.train()\n",
    "\n",
    "# Saves the model\n",
    "trainer.save_model(\"./distilbert_model\")\n",
    "\n",
    "# Evaluates the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Confusion Matrix\n",
    "predictions = trainer.predict(hf_dataset[\"test\"])\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Print overall performance\n",
    "print(\"\\n DistilBERT Performance:\")\n",
    "print(f\"Model achieved an accuracy of {results['eval_accuracy']*100:.2f}% on the validation dataset.\")\n",
    "\n",
    "# Print per-class performance\n",
    "class_metrics = results[\"eval_class_metrics\"]\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    print(f\"Class {class_name}: Precision={metrics['Precision']*100:.2f}%, Recall={metrics['Recall']*100:.2f}%, F1-score={metrics['F1-score']*100:.2f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Save confusion matrix to a CSV\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Negative\", \"Neutral\", \"Positive\"], columns=[\"Predicted Negative\", \"Predicted Neutral\", \"Predicted Positive\"])\n",
    "conf_matrix_df.to_csv(\"confusion_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447720a-6831-4d37-8778-e793f83d9666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 (TensorFlow)",
   "language": "python",
   "name": "pyenv_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
